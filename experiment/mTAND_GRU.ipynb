{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6s-C6y6GA57"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Health Project/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJCY-_3nHGNG"
      },
      "source": [
        "# **Load embeddings, build cohort, save data for training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG3HrM7GkwCH"
      },
      "source": [
        "## Load libraries and setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-MoFA6NkkbZ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "!pip install tslearn\n",
        "!pip install minisom\n",
        "!pip install dtw-python\n",
        "!pip install Levenshtein\n",
        "!pip install optuna\n",
        "\n",
        "\n",
        "from tslearn.metrics import cdist_dtw\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from minisom import MiniSom\n",
        "from dtw import dtw\n",
        "\n",
        "from datetime import timedelta\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA #Principal Component Analysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from IPython.display import display, HTML, Image\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "# Access data using Google BigQuery.\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "\n",
        "import bigframes.pandas as bf\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "\n",
        "import copy\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyBV_Q9DkyD3"
      },
      "outputs": [],
      "source": [
        "bf.options.bigquery.location = \"US\"\n",
        "bf.options.bigquery.project = 'loyal-mason-431106-n3' #'hellobigquery-431508'\n",
        "\n",
        "# authenticate\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up environment variables\n",
        "project_id = 'loyal-mason-431106-n3' #'hellobigquery-431508'\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "dataset = 'mimiciv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiX3vZN1H8Kr"
      },
      "source": [
        "## **Load embedding mapping from itemid to vectors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV99B8kIGsQM"
      },
      "outputs": [],
      "source": [
        "# load embedding vectors\n",
        "df_itemid_to_vector = pd.read_csv(file_path + 'itemid_to_vector.csv')\n",
        "itemid_to_vector = {str(key): value for key, value in df_itemid_to_vector.set_index('itemid').T.to_dict('list').items()}\n",
        "print(len(itemid_to_vector))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzCzW87JgKC"
      },
      "source": [
        "## **Find patients with AD related ICD codes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9DdXgIB9d10"
      },
      "source": [
        "Load all diagnoses icd table, and filter with our event list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITNcxqhJ3KHP"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "  SELECT\n",
        "    d.*,\n",
        "    a.dischtime AS discharge_time\n",
        "  FROM\n",
        "    `physionet-data.mimiciv_hosp.diagnoses_icd` AS d\n",
        "  INNER JOIN\n",
        "    `physionet-data.mimiciv_hosp.admissions` AS a\n",
        "  ON\n",
        "    d.hadm_id = a.hadm_id\n",
        "  WHERE\n",
        "    d.icd_code IN ('G300', 'G301', 'G308', 'G309', 'F0280', 'F0281', 'F0290', 'F0391', 'F04',\n",
        "      'F060', 'F068', 'G3101', 'G3109', 'G311', 'G3183', 'G3185', 'G3189', 'G319',\n",
        "      'G454', 'G937', 'G94', 'G910', 'G911', 'G912', 'F0150', 'F0151', 'I675',\n",
        "      'I671', 'I672', 'I674', 'I676', 'I677', 'I6781', 'I6782', 'I6789', 'I679')\n",
        "\n",
        "\"\"\"\n",
        "df_ad_patients_with_discharge_time = bf.read_gbq(query)\n",
        "print(len(df_ad_patients_with_discharge_time))\n",
        "df_ad_patients_with_discharge_time.head(10)\n",
        "\n",
        "# WHERE\n",
        "    # d.icd_code IN ('G300', 'G301', 'G308', 'G309', 'F0280', 'F0281', 'F0290', 'F0391', 'F04',\n",
        "    # 'F060', 'F068', 'G3101', 'G3109', 'G311', 'G3183', 'G3185', 'G3189', 'G319',\n",
        "    # 'G454', 'G937', 'G94', 'G910', 'G911', 'G912', 'F0150', 'F0151', 'I675',\n",
        "    # 'I671', 'I672', 'I674', 'I676', 'I677', 'I6781', 'I6782', 'I6789', 'I679',\n",
        "    # '29012', '331', '2904', '29041', '29042', '29043', '294', '2948', '2949',\n",
        "    # '2941', '29411', '2942', '29421', '3312', '3316', '3317', '33111', '33119',\n",
        "    # '33181', '33182', '33189', '4377', '3313', '3314', '3315', '437', '4371',\n",
        "    # '4372', '4373', '4374', '4375', '4376', '4378', '4379', '33183', '331',\n",
        "    # '3311' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCcBGB_t-MKy"
      },
      "outputs": [],
      "source": [
        "# df_ad_patients_with_discharge_time.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fio1IFE6NG6"
      },
      "outputs": [],
      "source": [
        "ad_patients_list = list(df_ad_patients_with_discharge_time.drop_duplicates(subset=['subject_id'])['subject_id'])\n",
        "len(ad_patients_list)\n",
        "# df_ad_patients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m0Qy9ZsYLPW"
      },
      "source": [
        "## Possible Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFQoT_cLBSPb"
      },
      "source": [
        "## **See what tests are taken by these patients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRvB4a1DYy48"
      },
      "outputs": [],
      "source": [
        "# See what labtests are taken for these patients\n",
        "\n",
        "query = \"\"\"\n",
        "  SELECT\n",
        "    d.subject_id,\n",
        "    d.hadm_id,\n",
        "    d.icd_code,\n",
        "    d.icd_version,\n",
        "    l.itemid,\n",
        "    l.valuenum,\n",
        "    l.valueuom,\n",
        "    l.labevent_id,\n",
        "    l.charttime,\n",
        "    l.flag,\n",
        "    dlab.label,\n",
        "    dlab.fluid,\n",
        "    dlab.category\n",
        "  FROM\n",
        "    (\n",
        "      SELECT subject_id, hadm_id, icd_code, icd_version\n",
        "      FROM `physionet-data.mimiciv_hosp.diagnoses_icd`\n",
        "      WHERE icd_code IN ('G300', 'G301', 'G308', 'G309', 'F0280', 'F0281', 'F0290', 'F0391', 'F04',\n",
        "            'F060', 'F068', 'G3101', 'G3109', 'G311', 'G3183', 'G3185', 'G3189', 'G319',\n",
        "            'G454', 'G937', 'G94', 'G910', 'G911', 'G912', 'F0150', 'F0151', 'I675',\n",
        "            'I671', 'I672', 'I674', 'I676', 'I677', 'I6781', 'I6782', 'I6789', 'I679')\n",
        "    ) AS d\n",
        "  INNER JOIN\n",
        "    (\n",
        "      SELECT subject_id, hadm_id, labevent_id, itemid, valuenum, valueuom, charttime, flag\n",
        "      FROM `physionet-data.mimiciv_hosp.labevents`\n",
        "      WHERE lower(flag) LIKE 'abnormal%'\n",
        "        OR flag IS NULL\n",
        "    ) AS l\n",
        "  ON\n",
        "    d.subject_id = l.subject_id\n",
        "    AND d.hadm_id = l.hadm_id\n",
        "  INNER JOIN\n",
        "    `physionet-data.mimiciv_hosp.d_labitems` AS dlab\n",
        "  ON\n",
        "    l.itemid = dlab.itemid\n",
        "    WHERE l.valuenum IS NOT NULL  -- remove NULL values\n",
        "  # WHERE (LOWER(dlab.label) LIKE '%csf' OR\n",
        "  #  LOWER(dlab.label) LIKE'%b12%')\n",
        "\"\"\"\n",
        "df_ad_patients_lab_results = bf.read_gbq(query).sort_values(by=['subject_id', 'charttime', 'labevent_id']).reset_index(drop=True)\n",
        "print(len(df_ad_patients_lab_results))\n",
        "df_ad_patients_lab_results.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBC0vKWipNT8"
      },
      "source": [
        "For each patient, only keep the test data with charttime before their diagnoses' dischargetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVvr1CWxpvd3"
      },
      "outputs": [],
      "source": [
        "df_temp = df_ad_patients_with_discharge_time[['subject_id', 'hadm_id', 'discharge_time']].drop_duplicates(subset=['subject_id', 'hadm_id']).to_pandas()\n",
        "df_ad_patients_lab_results_pd = df_ad_patients_lab_results.to_pandas()\n",
        "\n",
        "df_ad_patients_lab_results_pd = pd.merge(df_ad_patients_lab_results_pd, df_temp, on=['subject_id', 'hadm_id'], how='left')\n",
        "\n",
        "df_ad_patients_lab_results_pd = df_ad_patients_lab_results_pd[df_ad_patients_lab_results_pd['charttime'] <= df_ad_patients_lab_results_pd['discharge_time']].reset_index(drop=True)\n",
        "\n",
        "print(len(df_ad_patients_lab_results_pd))\n",
        "df_ad_patients_lab_results_pd.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FmEmITGeIFc"
      },
      "outputs": [],
      "source": [
        "df_ad_patients_lab_results_pd = df_ad_patients_lab_results.to_pandas()\n",
        "df_ad_patients_lab_results_pd['itemid'] = df_ad_patients_lab_results_pd['itemid'].astype(str)\n",
        "df_ad_patients_lab_results_pd.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCI3ezaW5_3B"
      },
      "outputs": [],
      "source": [
        "# see how many unique patients:\n",
        "print(len(df_ad_patients_lab_results_pd['subject_id'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EemiR1TiZQ23"
      },
      "outputs": [],
      "source": [
        "possible_tests = bf.read_gbq(\"\"\"\n",
        "  SELECT *\n",
        "  FROM `physionet-data.mimiciv_hosp.d_labitems`\n",
        "\"\"\")\n",
        "ad_tests = list(set(df_ad_patients_lab_results['itemid']))\n",
        "ad_test_names = possible_tests[possible_tests['itemid'].isin(ad_tests)].to_pandas()\n",
        "ad_test_names['itemid'] = ad_test_names['itemid'].astype(str)\n",
        "\n",
        "ad_test_names.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXE_VJKUek00"
      },
      "outputs": [],
      "source": [
        "itemid_counts_df = df_ad_patients_lab_results_pd.groupby('itemid')['subject_id'].nunique().reset_index()\n",
        "itemid_counts_df.columns = ['itemid', 'count']\n",
        "itemid_counts_df['itemid'] = itemid_counts_df['itemid'].astype(str)\n",
        "\n",
        "\n",
        "merged_df = pd.merge(itemid_counts_df, ad_test_names, on='itemid', how='inner')\n",
        "\n",
        "merged_df_sorted = merged_df.sort_values(by='count', ascending=False)\n",
        "\n",
        "merged_df_sorted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbG2nL3MKH3q"
      },
      "source": [
        "### **Apply Word2vec embedding, treating each patient's test history as a \"sentence\" and each labtest as a \"word\".**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w-3D1b_KcM_"
      },
      "source": [
        "Prepare the sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pxtv-jjetD8"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "  SELECT\n",
        "    subject_id,\n",
        "    STRING_AGG(CAST(itemid AS STRING) ORDER BY charttime ASC) AS itemid_sequence,  -- ASC for time order\n",
        "    ARRAY_AGG(CAST(valuenum AS FLOAT64) ORDER BY charttime ASC) AS test_value_sequence,  -- ASC for time order\n",
        "    ARRAY_LENGTH(ARRAY_AGG(itemid ORDER BY charttime ASC)) AS sequence_length,\n",
        "    STRING_AGG(DISTINCT icd_code ORDER BY icd_code ASC) AS icd_codes,\n",
        "    'mimic_iv' AS data_source,  -- New column with constant value 'mimic_iv'\n",
        "    CASE\n",
        "        WHEN REGEXP_CONTAINS(STRING_AGG(DISTINCT filtered.icd_code ORDER BY filtered.icd_code ASC), r'G30.*')\n",
        "        OR REGEXP_CONTAINS(STRING_AGG(DISTINCT filtered.icd_code ORDER BY filtered.icd_code ASC), r'331.*')\n",
        "        THEN 1\n",
        "        ELSE 0\n",
        "    END AS label_ad  -- New column that marks Alzheimer's Disease based on G30 ICD codes\n",
        "  FROM (\n",
        "    SELECT\n",
        "        d.subject_id,\n",
        "        l.itemid,\n",
        "        l.valuenum,\n",
        "        l.charttime,\n",
        "        d.icd_code,\n",
        "        ROW_NUMBER() OVER (PARTITION BY d.subject_id, l.itemid ORDER BY l.charttime ASC) AS rn\n",
        "    FROM\n",
        "        physionet-data.mimiciv_hosp.diagnoses_icd AS d\n",
        "    JOIN\n",
        "        physionet-data.mimiciv_hosp.labevents AS l\n",
        "    ON\n",
        "        d.subject_id = l.subject_id\n",
        "    JOIN\n",
        "        physionet-data.mimiciv_hosp.admissions AS a\n",
        "    ON\n",
        "        l.subject_id = a.subject_id\n",
        "    JOIN\n",
        "      physionet-data.mimiciv_hosp.d_labitems AS dlab\n",
        "    ON\n",
        "      l.itemid = dlab.itemid\n",
        "    WHERE\n",
        "      (\n",
        "        d.icd_code LIKE 'G30%' OR\n",
        "        d.icd_code LIKE 'F01%' OR\n",
        "        d.icd_code LIKE 'F03%' OR\n",
        "        d.icd_code LIKE 'F02%' OR\n",
        "        d.icd_code LIKE 'R54%' OR\n",
        "\n",
        "        d.icd_code IN ('G318', 'G310', 'G311', 'G318', 'G319', '3310', '3311', '3312', '3319', '2904', '2900', '2901', '2902', '2903', '2908', '2909', '797')\n",
        "      )\n",
        "      AND l.valuenum IS NOT NULL\n",
        "      # AND l.charttime <= a.dischtime\n",
        "      AND l.hadm_id < a.hadm_id  -- Only include hadm_id before first AD diagnosis\n",
        "      AND (LOWER(l.flag) LIKE 'abnormal%' OR l.flag IS NULL)\n",
        "      # AND (LOWER(l.flag) LIKE 'abnormal%')\n",
        "      AND lower(dlab.fluid) LIKE '%blood%'\n",
        "\n",
        "  ) AS filtered\n",
        "  WHERE\n",
        "    rn <= 2  -- only keep the first two entries for each itemid per patient\n",
        "  GROUP BY\n",
        "    subject_id\n",
        "  ORDER BY\n",
        "    subject_id;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_itemid_sequences_4 = bf.read_gbq(query)\n",
        "print(len(df_itemid_sequences_4))\n",
        "df_itemid_sequences_4.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkGLbJj3rNFZ"
      },
      "outputs": [],
      "source": [
        "# prompt: plot distribution of df_itemid_sequences['sequence_length']\n",
        "df_labtest_sequences_pd_4 = df_itemid_sequences_4.to_pandas()\n",
        "sns.displot(df_labtest_sequences_pd_4['sequence_length'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3Q-tFk0G3ie"
      },
      "outputs": [],
      "source": [
        "print(df_labtest_sequences_pd_4['sequence_length'].max())\n",
        "print(df_labtest_sequences_pd_4['sequence_length'].min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWZ8poPElPoX"
      },
      "outputs": [],
      "source": [
        "print(df_labtest_sequences_pd_4.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ALfTLmAPvT"
      },
      "source": [
        "### **Similarly for MIMIC_III**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH0f0javAVNb"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "  SELECT\n",
        "    subject_id,\n",
        "    STRING_AGG(CAST(itemid AS STRING) ORDER BY charttime ASC) AS itemid_sequence,  -- ASC for time order\n",
        "    ARRAY_AGG(CAST(valuenum AS FLOAT64) ORDER BY charttime ASC) AS test_value_sequence,  -- ASC for time order\n",
        "    ARRAY_LENGTH(ARRAY_AGG(itemid ORDER BY charttime ASC)) AS sequence_length,\n",
        "    STRING_AGG(DISTINCT icd9_code ORDER BY icd9_code ASC) AS icd_codes,\n",
        "    'mimic_iii' AS data_source,\n",
        "    CASE WHEN STRING_AGG(DISTINCT icd9_code ORDER BY icd9_code ASC) LIKE '331%' THEN 1 ELSE 0 END AS label_ad\n",
        "  FROM (\n",
        "    SELECT\n",
        "      d.subject_id,\n",
        "      l.itemid,\n",
        "      l.valuenum,\n",
        "      l.charttime,\n",
        "      d.icd9_code,\n",
        "      ROW_NUMBER() OVER (PARTITION BY d.subject_id, l.itemid ORDER BY l.charttime ASC) AS rn\n",
        "    FROM\n",
        "        `physionet-data.mimiciii_clinical.diagnoses_icd` AS d\n",
        "    JOIN\n",
        "        `physionet-data.mimiciii_clinical.labevents` AS l\n",
        "    ON\n",
        "        d.subject_id = l.subject_id\n",
        "    JOIN\n",
        "        `physionet-data.mimiciii_clinical.admissions` AS a\n",
        "    ON\n",
        "        l.subject_id = a.subject_id\n",
        "    INNER JOIN\n",
        "      `physionet-data.mimiciii_clinical.d_labitems` AS dlab\n",
        "    ON\n",
        "      l.itemid = dlab.itemid\n",
        "    WHERE\n",
        "      d.icd9_code LIKE '331%'  -- ICD-9 codes for Alzheimer's and related diseases\n",
        "      AND l.valuenum IS NOT NULL\n",
        "      # AND l.charttime <= a.dischtime\n",
        "      AND l.hadm_id < a.hadm_id  -- Only include hadm_id before first AD diagnosis\n",
        "      AND (LOWER(l.flag) LIKE 'abnormal%' OR l.flag IS NULL)\n",
        "      # AND (LOWER(l.flag) LIKE 'abnormal%')\n",
        "      AND lower(dlab.fluid) LIKE '%blood%'\n",
        "\n",
        "\n",
        "  ) AS filtered\n",
        "  WHERE\n",
        "    rn <= 2  -- only keep the first two entries for each itemid per patient\n",
        "  GROUP BY\n",
        "    subject_id\n",
        "  ORDER BY\n",
        "    subject_id;\n",
        "\"\"\"\n",
        "df_itemid_sequences_3 = bf.read_gbq(query)\n",
        "print(len(df_itemid_sequences_3))\n",
        "df_itemid_sequences_3.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-53hElVWY-NI"
      },
      "outputs": [],
      "source": [
        "df_labtest_sequences_pd_3 = df_itemid_sequences_3.to_pandas()\n",
        "sns.displot(df_labtest_sequences_pd_3['sequence_length'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTK9PJUeITz5"
      },
      "outputs": [],
      "source": [
        "df_labtest_sequences_pd_3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ-UD5_rzLmT"
      },
      "source": [
        "**Combine data fromo mimic iii and iv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiFGRXVRzLAw"
      },
      "outputs": [],
      "source": [
        "# df_labtest_sequences_pd = df_itemid_sequences.to_pandas()\n",
        "df_labtest_sequences_pd = pd.concat([df_labtest_sequences_pd_4, df_labtest_sequences_pd_3], ignore_index=True)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "\n",
        "sns.displot(df_labtest_sequences_pd['sequence_length'], kde=True, color='#00BFFF')\n",
        "\n",
        "skewness = df_labtest_sequences_pd['sequence_length'].skew()\n",
        "print(f'Skewness: {skewness}')\n",
        "\n",
        "plt.title(f'Sequence Length Distribution with Skewness = {skewness:.2f}')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEwuUORJ2_qi"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df_labtest_sequences_pd)):\n",
        "  seq = df_labtest_sequences_pd.loc[i, 'itemid_sequence']\n",
        "  if '52285' in seq:\n",
        "    print(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oVwLhgfSDJp"
      },
      "source": [
        "**Split itemid sequences, save dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8robhc6SAKa"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "df_labtest_sequences_pd['itemid_sequence'] = df_labtest_sequences_pd['itemid_sequence'].apply(lambda x: x.split(',') if isinstance(x, str) else [])\n",
        "# df_labtest_sequences_pd['test_value_sequence'] = df_labtest_sequences_pd['test_value_sequence'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "# df_labtest_sequences_pd.to_csv(file_path + 'df_labtest_sequences_pd.csv', index=False)\n",
        "\n",
        "# patients_itemid_seqs = df_labtest_sequences_pd['itemid_sequence']\n",
        "# patients_itemid_seqs[:1]\n",
        "df_labtest_sequences_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mis4Rv4nzoNQ"
      },
      "outputs": [],
      "source": [
        "# df_labtest_sequences_pd = pd.read_csv('df_labtest_sequences_pd.csv')\n",
        "print(len(df_labtest_sequences_pd))\n",
        "df_labtest_sequences_pd.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqJq6fJx445S"
      },
      "outputs": [],
      "source": [
        "df_labtest_sequences_pd.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao8IpKxPGkLY"
      },
      "source": [
        "# **Apply scalers for each labtest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVRco7n7zAD"
      },
      "source": [
        "**Train scalers for each labtest itemid based on global data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaWHKvfr7x27"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# labtest_scalers = {}\n",
        "# outlier_ranges = []\n",
        "\n",
        "# unique_itemids = df_ad_patients_lab_results_pd['itemid'].unique()\n",
        "# num_itemids = len(unique_itemids)\n",
        "\n",
        "# for i in range(num_itemids):\n",
        "#   itemid = unique_itemids[i]\n",
        "\n",
        "#   item_values = df_ad_patients_lab_results_pd[df_ad_patients_lab_results_pd['itemid'] == itemid]['valuenum'].dropna()\n",
        "#   # print(f\"Processing Itemid: {itemid}, values: {item_values}\")\n",
        "\n",
        "#   if item_values.empty:\n",
        "#     print(f\"Itemid: {itemid} has no valid data, skipping.\")\n",
        "#     continue\n",
        "\n",
        "\n",
        "#   if item_values.size == 0:\n",
        "#     print(f\"Itemid: {itemid} has no valid data, skipping.\")\n",
        "#     continue\n",
        "\n",
        "#   sys.stdout.write(f\"\\rProcessing itemid {itemid} , {i + 1}/{num_itemids} ({(i + 1) / num_itemids * 100:.2f}%)\")\n",
        "#   sys.stdout.flush()\n",
        "\n",
        "#   item_values_array = item_values.values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "#   scaler = RobustScaler()\n",
        "#   scaler.fit(item_values_array)\n",
        "\n",
        "#   labtest_scalers[itemid] = scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97PZOMABOu-N"
      },
      "outputs": [],
      "source": [
        "# print(len(labtest_scalers))\n",
        "# print(labtest_scalers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXaseeEzHomi"
      },
      "source": [
        "**save scaler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tvXr_VT_uw_"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open(file_path + 'labtest_scalers.pkl', 'wb') as file:\n",
        "#     pickle.dump(labtest_scalers, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPminjm0lMJ9"
      },
      "source": [
        "**Concatenate vectors of patients to matrices**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "soTQ7CGMlWR6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "\n",
        "df_all = copy.deepcopy(df_labtest_sequences_pd)\n",
        "\n",
        "# df_all['itemid_sequence'] = df_all['itemid_sequence'].apply(lambda x: list(x) if isinstance(x, list) else x)\n",
        "df_all['test_value_sequence'] = df_all['test_value_sequence'].apply(lambda x: list(x) if isinstance(x, list) else x)\n",
        "\n",
        "\n",
        "df_all['matrix'] = None\n",
        "df_all['scaled_value_sequence'] = None\n",
        "\n",
        "missing_scaler_count = defaultdict(int)\n",
        "missing_pca_vector_count = defaultdict(int)\n",
        "outlier_count = defaultdict(int)\n",
        "\n",
        "num_patients = len(df_all)\n",
        "\n",
        "for i in range(len(df_all)):\n",
        "\n",
        "  patient_matrix = []\n",
        "  updated_itemid_sequence = []\n",
        "  updated_test_value_sequence = []\n",
        "  scaled_values = []\n",
        "\n",
        "  sys.stdout.write(f\"\\rProcessing patient {i + 1}/{num_patients} ({(i + 1) / num_patients * 100:.2f}%)\")\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  itemid_sequence = df_all.loc[i, 'itemid_sequence']\n",
        "  test_value_sequence = df_all.loc[i, 'test_value_sequence']\n",
        "  # print(itemid_sequence)\n",
        "  # print(test_value_sequence)\n",
        "\n",
        "  # Iterate through itemid and test_value\n",
        "  for itemid, test_value in zip(itemid_sequence, test_value_sequence):\n",
        "    if itemid == '52285':\n",
        "      print(itemid, test_value)\n",
        "    #\n",
        "    # print(itemid, test_value)\n",
        "    # scaler = labtest_scalers.get(itemid)\n",
        "    # if scaler is None:\n",
        "    #     missing_scaler_count[itemid] += 1\n",
        "    #     continue  # Skip if no scaler for the itemid\n",
        "\n",
        "    # item_range = df_outlier_ranges[df_outlier_ranges['itemid'] == itemid]\n",
        "    # # print(item_range)\n",
        "    # lower_bound = item_range['lower_bound'].values[0] if not item_range.empty else None\n",
        "    # upper_bound = item_range['upper_bound'].values[0] if not item_range.empty else None\n",
        "\n",
        "    # if lower_bound is not None and upper_bound is not None:\n",
        "    #     if test_value < lower_bound or test_value > upper_bound:\n",
        "    #         outlier_count[itemid] += 1\n",
        "    #         continue  # Skip if the test value is outside the outlier range\n",
        "    # else:\n",
        "    #     print(f\"No outlier range found for itemid: {itemid}\")\n",
        "    #     continue\n",
        "\n",
        "\n",
        "    pca_vector = itemid_to_vector.get(str(itemid))\n",
        "    if pca_vector is None:\n",
        "        missing_pca_vector_count[itemid] += 1\n",
        "        continue  # Skip if no PCA vector for the itemid\n",
        "\n",
        "    # Combine the 20-dim PCA vector with the test value to form a 21-dim vector\n",
        "    test_value_array = np.array(test_value).reshape(-1, 1)\n",
        "\n",
        "    # test_value_scaled = scaler.transform(test_value_array)\n",
        "    # print(test_value, test_value_scaled)\n",
        "\n",
        "    # combined_vector = np.append(pca_vector, test_value_scaled)\n",
        "    combined_vector = np.append(pca_vector, test_value)\n",
        "\n",
        "    patient_matrix.append(combined_vector)\n",
        "    # print(combined_vector)\n",
        "\n",
        "    # Update sequences\n",
        "    updated_itemid_sequence.append(itemid)\n",
        "    updated_test_value_sequence.append(test_value)\n",
        "    # scaled_values.append(test_value_scaled.item())\n",
        "\n",
        "  # Update the patient's matrix and sequences in the DataFrame\n",
        "  df_all.at[i, 'matrix'] = patient_matrix\n",
        "  df_all.at[i, 'itemid_sequence'] = updated_itemid_sequence\n",
        "  df_all.at[i, 'test_value_sequence'] = updated_test_value_sequence\n",
        "  df_all.at[i, 'sequence_length'] = len(updated_itemid_sequence)\n",
        "  # df_all.at[i, 'scaled_value_sequence'] = scaled_values\n",
        "\n",
        "print('\\n')\n",
        "print(df_all.head(1))\n",
        "print(f\"Number of patients: {len(df_all)}\")\n",
        "# print(f\"Number of values with missing scaler: {missing_scaler_count}\")\n",
        "print(f\"Number of values with missing pca vector: {missing_pca_vector_count}\")\n",
        "print(f\"Number of values outside outlier range: {outlier_count}\")\n",
        "\n",
        "df_all = df_all[df_all['sequence_length'] >= 4].reset_index(drop=True)\n",
        "df_all.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlFI1Z3TJCz7"
      },
      "source": [
        "# **save matrix df**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpe82xLwH-Vn"
      },
      "outputs": [],
      "source": [
        "# df_all['matrix'] = df_all['matrix'].apply(lambda x: str(x))\n",
        "# df_all.to_csv(file_path + 'df_labtest_sequences_pd_pre_blood.csv', index=False)\n",
        "# np.save(file_path + 'matrix_data_pre_blood.npy', df_all['matrix'].values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_all = pd.read_csv(file_path + 'df_labtest_sequences_pd_pre_blood.csv')\n",
        "# df_all['matrix'] = df_all['matrix'].apply(lambda x: np.array(eval(x, {'array': np.array})))\n"
      ],
      "metadata": {
        "id": "SyuZlGUUjhdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szMI_SfHy7oz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# df_labtest_sequences_pd = copy.deepcopy(df_all)\n",
        "# df_all.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU**"
      ],
      "metadata": {
        "id": "xOnkYpSz66-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_labtest_GRU = copy.deepcopy(df_all)\n",
        "df_labtest_GRU.head()"
      ],
      "metadata": {
        "id": "iIZPL9Wk_iRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.best_model = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "\n",
        "class BalancedFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        batch_size = targets.size(0)\n",
        "        class_weights = torch.bincount(targets).float()\n",
        "        class_weights = batch_size / (2 * class_weights)\n",
        "        sample_weights = class_weights[targets]\n",
        "        focal_loss = sample_weights * self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class EnhancedMedicalGRUDelta(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size * 2),\n",
        "            nn.LayerNorm(hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            ResidualBlock(hidden_size)\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_prob,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size * 2,\n",
        "            num_heads=16,\n",
        "            dropout=dropout_prob\n",
        "        )\n",
        "\n",
        "        self.residual_conn = nn.Linear(input_size, hidden_size * 2)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_size, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_lengths = [len(seq) for seq in x]\n",
        "        x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
        "        batch_size = x_padded.size(0)\n",
        "\n",
        "        x_reshaped = x_padded.view(-1, x_padded.size(-1))\n",
        "        x_features = self.feature_extraction(x_reshaped)\n",
        "        x_features = x_features.view(batch_size, -1, self.hidden_size)\n",
        "\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
        "            x_features,\n",
        "            x_lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        packed_output, _ = self.gru(packed_input)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        residual = self.residual_conn(x_padded)\n",
        "\n",
        "        attention_output, _ = self.attention(\n",
        "            output.transpose(0, 1),\n",
        "            output.transpose(0, 1),\n",
        "            output.transpose(0, 1)\n",
        "        )\n",
        "        attention_output = attention_output.transpose(0, 1)\n",
        "        attention_output = attention_output + residual\n",
        "\n",
        "        mask = torch.arange(output.size(1))[None, :] < torch.tensor(x_lengths)[:, None]\n",
        "        mask = mask.to(output.device)\n",
        "        masked_output = attention_output * mask.unsqueeze(-1)\n",
        "        pooled = masked_output.sum(dim=1) / mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "def prepare_medical_data(df):\n",
        "    train_df, eval_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.1,\n",
        "        random_state=42,\n",
        "        stratify=df['label_ad']\n",
        "    )\n",
        "\n",
        "    true_eval_df = eval_df[eval_df['label_ad'] == 1]\n",
        "    false_eval_df = eval_df[eval_df['label_ad'] == 0]\n",
        "    downsampled_false_eval_df = false_eval_df.sample(n=len(true_eval_df), random_state=42)\n",
        "    eval_df = pd.concat([true_eval_df, downsampled_false_eval_df])\n",
        "\n",
        "    def process_sequence(matrix):\n",
        "        if not isinstance(matrix, np.ndarray):\n",
        "            matrix = np.array(matrix)\n",
        "        return torch.tensor(matrix, dtype=torch.float32)\n",
        "\n",
        "    X_train_tensor = [process_sequence(row['matrix']) for _, row in train_df.iterrows()]\n",
        "    y_train_tensor = torch.tensor(train_df['label_ad'].values, dtype=torch.long)\n",
        "\n",
        "    X_eval_tensor = [process_sequence(row['matrix']) for _, row in eval_df.iterrows()]\n",
        "    y_eval_tensor = torch.tensor(eval_df['label_ad'].values, dtype=torch.long)\n",
        "\n",
        "    return X_train_tensor, y_train_tensor, X_eval_tensor, y_eval_tensor\n",
        "\n",
        "def train_model(model, X_train, y_train, X_eval, y_eval, config):\n",
        "    criterion = BalancedFocalLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "    early_stopping = EarlyStopping(patience=config['patience'])\n",
        "\n",
        "    train_losses, eval_losses = [], []\n",
        "    train_accs, eval_accs = [], []\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        permutation = torch.randperm(len(X_train))\n",
        "        for i in range(0, len(X_train), config['batch_size']):\n",
        "            optimizer.zero_grad()\n",
        "            indices = permutation[i:i + config['batch_size']]\n",
        "            batch_X = [X_train[idx] for idx in indices]\n",
        "            batch_y = y_train[indices]\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += predicted.eq(batch_y).sum().item()\n",
        "\n",
        "        train_loss = train_loss / (len(X_train) // config['batch_size'])\n",
        "        train_acc = correct / total\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            eval_outputs = model(X_eval)\n",
        "            eval_loss = criterion(eval_outputs, y_eval).item()\n",
        "            _, predicted = eval_outputs.max(1)\n",
        "            eval_acc = predicted.eq(y_eval).sum().item() / len(y_eval)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        eval_losses.append(eval_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        eval_accs.append(eval_acc)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]} - '\n",
        "              f'Train Loss: {train_loss:.4f}, '\n",
        "              f'Train Acc: {train_acc:.4f}, '\n",
        "              f'Val Loss: {eval_loss:.4f}, '\n",
        "              f'Val Acc: {eval_acc:.4f}, '\n",
        "              f'LR: {current_lr:.6f}')\n",
        "\n",
        "        scheduler.step(eval_loss)\n",
        "        if early_stopping(eval_loss, model):\n",
        "            print(\"Early stopping triggered\")\n",
        "            model.load_state_dict(early_stopping.best_model)\n",
        "            break\n",
        "\n",
        "    return model, train_losses, eval_losses, train_accs, eval_accs\n",
        "\n",
        "config = {\n",
        "    'hidden_size': 256,\n",
        "    'num_layers': 3,\n",
        "    'dropout_prob': 0.5,\n",
        "    'learning_rate': 0.001,\n",
        "    'weight_decay': 0.01,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': 10,\n",
        "    'patience': 5\n",
        "}\n",
        "\n",
        "X_train, y_train, X_eval, y_eval = prepare_medical_data(df_labtest_GRU)\n",
        "config['input_size'] = X_train[0].shape[1]\n",
        "\n",
        "model = EnhancedMedicalGRUDelta(**{k: v for k, v in config.items()\n",
        "                                  if k in ['input_size', 'hidden_size',\n",
        "                                         'num_layers', 'dropout_prob']})\n",
        "\n",
        "model, train_losses, eval_losses, train_accs, eval_accs = train_model(\n",
        "    model, X_train, y_train, X_eval, y_eval, config\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(eval_losses, label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Acc')\n",
        "plt.plot(eval_accs, label='Val Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_eval)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    accuracy = (predicted == y_eval).sum().item() / len(y_eval)\n",
        "    print(f\"Final Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_eval, predicted)}\")\n",
        "    print(f\"Precision: {precision_score(y_eval, predicted):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_eval, predicted):.4f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_eval, predicted):.4f}\")\n",
        "\n",
        "    probs = F.softmax(outputs, dim=1)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_eval, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "l6a9lBNm_ibG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}