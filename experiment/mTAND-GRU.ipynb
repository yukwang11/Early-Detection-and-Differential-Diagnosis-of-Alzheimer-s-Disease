{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":19543,"status":"error","timestamp":1730451808542,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"},"user_tz":-660},"id":"V6s-C6y6GA57","outputId":"7be398ec-7432-4047-f7ec-a0c737b8a87d"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-8478ce49e1ee>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Health Project/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = '/content/drive/MyDrive/Health Project/'"]},{"cell_type":"markdown","metadata":{"id":"KJCY-_3nHGNG"},"source":["# **Load embeddings, build cohort, save data for training**"]},{"cell_type":"markdown","metadata":{"id":"rG3HrM7GkwCH"},"source":["## Load libraries and setup environment"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1730451786616,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"},"user_tz":-660},"id":"s-MoFA6NkkbZ"},"outputs":[],"source":["# Import libraries\n","\n","!pip install tslearn\n","!pip install minisom\n","!pip install dtw-python\n","!pip install Levenshtein\n","!pip install optuna\n","\n","\n","from tslearn.metrics import cdist_dtw\n","from sklearn.cluster import AgglomerativeClustering\n","from minisom import MiniSom\n","from dtw import dtw\n","\n","from datetime import timedelta\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import os\n","\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import DBSCAN\n","from sklearn import metrics\n","from sklearn.decomposition import PCA #Principal Component Analysis\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","\n","import re\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","\n","from IPython.display import display, HTML, Image\n","%matplotlib inline\n","\n","plt.style.use('ggplot')\n","plt.rcParams.update({'font.size': 20})\n","\n","# Access data using Google BigQuery.\n","from google.colab import auth\n","from google.cloud import bigquery\n","\n","import bigframes.pandas as bf\n","import matplotlib.pyplot as plt\n","import plotly as py\n","import plotly.graph_objs as go\n","from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","import plotly.io as pio\n","pio.renderers.default = \"colab\"\n","\n","from gensim.models import Word2Vec\n","\n","from IPython.display import clear_output\n","\n","import torch\n","\n","import copy\n","import datetime\n","import sys\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyBV_Q9DkyD3","executionInfo":{"status":"aborted","timestamp":1730451786616,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["bf.options.bigquery.location = \"US\"\n","bf.options.bigquery.project = 'loyal-mason-431106-n3' #'hellobigquery-431508'\n","\n","# authenticate\n","auth.authenticate_user()\n","\n","# Set up environment variables\n","project_id = 'loyal-mason-431106-n3' #'hellobigquery-431508'\n","os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n","dataset = 'mimiciv'\n"]},{"cell_type":"markdown","metadata":{"id":"jiX3vZN1H8Kr"},"source":["## **Load embedding mapping from itemid to vectors**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yV99B8kIGsQM","executionInfo":{"status":"aborted","timestamp":1730451786616,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# load embedding vectors\n","df_itemid_to_vector = pd.read_csv(file_path + 'itemid_to_vector.csv')\n","itemid_to_vector = {str(key): value for key, value in df_itemid_to_vector.set_index('itemid').T.to_dict('list').items()}\n","print(len(itemid_to_vector))\n"]},{"cell_type":"markdown","metadata":{"id":"JpzCzW87JgKC"},"source":["## **Find patients with AD related ICD codes.**"]},{"cell_type":"markdown","metadata":{"id":"n9DdXgIB9d10"},"source":["Load all diagnoses icd table, and filter with our event list:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITNcxqhJ3KHP","executionInfo":{"status":"aborted","timestamp":1730451786616,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["query = \"\"\"\n","  SELECT\n","    d.*,\n","    a.dischtime AS discharge_time\n","  FROM\n","    `physionet-data.mimiciv_hosp.diagnoses_icd` AS d\n","  INNER JOIN\n","    `physionet-data.mimiciv_hosp.admissions` AS a\n","  ON\n","    d.hadm_id = a.hadm_id\n","  WHERE\n","    d.icd_code IN ('G300', 'G301', 'G308', 'G309', 'F0280', 'F0281', 'F0290', 'F0391', 'F04',\n","      'F060', 'F068', 'G3101', 'G3109', 'G311', 'G3183', 'G3185', 'G3189', 'G319',\n","      'G454', 'G937', 'G94', 'G910', 'G911', 'G912', 'F0150', 'F0151', 'I675',\n","      'I671', 'I672', 'I674', 'I676', 'I677', 'I6781', 'I6782', 'I6789', 'I679')\n","\n","\"\"\"\n","df_ad_patients_with_discharge_time = bf.read_gbq(query)\n","print(len(df_ad_patients_with_discharge_time))\n","df_ad_patients_with_discharge_time.head(10)\n","\n","# WHERE\n","    # d.icd_code IN ('G300', 'G301', 'G308', 'G309', 'F0280', 'F0281', 'F0290', 'F0391', 'F04',\n","    # 'F060', 'F068', 'G3101', 'G3109', 'G311', 'G3183', 'G3185', 'G3189', 'G319',\n","    # 'G454', 'G937', 'G94', 'G910', 'G911', 'G912', 'F0150', 'F0151', 'I675',\n","    # 'I671', 'I672', 'I674', 'I676', 'I677', 'I6781', 'I6782', 'I6789', 'I679',\n","    # '29012', '331', '2904', '29041', '29042', '29043', '294', '2948', '2949',\n","    # '2941', '29411', '2942', '29421', '3312', '3316', '3317', '33111', '33119',\n","    # '33181', '33182', '33189', '4377', '3313', '3314', '3315', '437', '4371',\n","    # '4372', '4373', '4374', '4375', '4376', '4378', '4379', '33183', '331',\n","    # '3311' )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCcBGB_t-MKy","executionInfo":{"status":"aborted","timestamp":1730451786616,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# df_ad_patients_with_discharge_time.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fio1IFE6NG6","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":11,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["ad_patients_list = list(df_ad_patients_with_discharge_time.drop_duplicates(subset=['subject_id'])['subject_id'])\n","len(ad_patients_list)\n","# df_ad_patients"]},{"cell_type":"markdown","metadata":{"id":"8m0Qy9ZsYLPW"},"source":["## Possible Tests"]},{"cell_type":"markdown","metadata":{"id":"bFQoT_cLBSPb"},"source":["## **See what tests are taken by these patients**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRvB4a1DYy48","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# See what labtests are taken for these patients\n","\n","query = \"\"\"\n","  SELECT\n","    d.subject_id,\n","    d.hadm_id,\n","    d.icd_code,\n","    d.icd_version,\n","    l.itemid,\n","    l.valuenum,\n","    l.valueuom,\n","    l.labevent_id,\n","    l.charttime,\n","    l.flag,\n","    dlab.label,\n","    dlab.fluid,\n","    dlab.category\n","  FROM\n","    (\n","      SELECT subject_id, hadm_id, icd_code, icd_version\n","      FROM `physionet-data.mimiciv_hosp.diagnoses_icd`\n","      WHERE icd_code IN ('G300', 'G301', 'G308', 'G309', 'F0280', 'F0281', 'F0290', 'F0391', 'F04',\n","            'F060', 'F068', 'G3101', 'G3109', 'G311', 'G3183', 'G3185', 'G3189', 'G319',\n","            'G454', 'G937', 'G94', 'G910', 'G911', 'G912', 'F0150', 'F0151', 'I675',\n","            'I671', 'I672', 'I674', 'I676', 'I677', 'I6781', 'I6782', 'I6789', 'I679')\n","    ) AS d\n","  INNER JOIN\n","    (\n","      SELECT subject_id, hadm_id, labevent_id, itemid, valuenum, valueuom, charttime, flag\n","      FROM `physionet-data.mimiciv_hosp.labevents`\n","      WHERE lower(flag) LIKE 'abnormal%'\n","        OR flag IS NULL\n","    ) AS l\n","  ON\n","    d.subject_id = l.subject_id\n","    AND d.hadm_id = l.hadm_id\n","  INNER JOIN\n","    `physionet-data.mimiciv_hosp.d_labitems` AS dlab\n","  ON\n","    l.itemid = dlab.itemid\n","    WHERE l.valuenum IS NOT NULL  -- remove NULL values\n","  # WHERE (LOWER(dlab.label) LIKE '%csf' OR\n","  #  LOWER(dlab.label) LIKE'%b12%')\n","\"\"\"\n","df_ad_patients_lab_results = bf.read_gbq(query).sort_values(by=['subject_id', 'charttime', 'labevent_id']).reset_index(drop=True)\n","print(len(df_ad_patients_lab_results))\n","df_ad_patients_lab_results.head(10)\n"]},{"cell_type":"markdown","metadata":{"id":"pBC0vKWipNT8"},"source":["For each patient, only keep the test data with charttime before their diagnoses' dischargetime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVvr1CWxpvd3","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["df_temp = df_ad_patients_with_discharge_time[['subject_id', 'hadm_id', 'discharge_time']].drop_duplicates(subset=['subject_id', 'hadm_id']).to_pandas()\n","df_ad_patients_lab_results_pd = df_ad_patients_lab_results.to_pandas()\n","\n","df_ad_patients_lab_results_pd = pd.merge(df_ad_patients_lab_results_pd, df_temp, on=['subject_id', 'hadm_id'], how='left')\n","\n","df_ad_patients_lab_results_pd = df_ad_patients_lab_results_pd[df_ad_patients_lab_results_pd['charttime'] <= df_ad_patients_lab_results_pd['discharge_time']].reset_index(drop=True)\n","\n","print(len(df_ad_patients_lab_results_pd))\n","df_ad_patients_lab_results_pd.head(10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FmEmITGeIFc","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["df_ad_patients_lab_results_pd = df_ad_patients_lab_results.to_pandas()\n","df_ad_patients_lab_results_pd['itemid'] = df_ad_patients_lab_results_pd['itemid'].astype(str)\n","df_ad_patients_lab_results_pd.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCI3ezaW5_3B","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# see how many unique patients:\n","print(len(df_ad_patients_lab_results_pd['subject_id'].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EemiR1TiZQ23","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["possible_tests = bf.read_gbq(\"\"\"\n","  SELECT *\n","  FROM `physionet-data.mimiciv_hosp.d_labitems`\n","\"\"\")\n","ad_tests = list(set(df_ad_patients_lab_results['itemid']))\n","ad_test_names = possible_tests[possible_tests['itemid'].isin(ad_tests)].to_pandas()\n","ad_test_names['itemid'] = ad_test_names['itemid'].astype(str)\n","\n","ad_test_names.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXE_VJKUek00","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["itemid_counts_df = df_ad_patients_lab_results_pd.groupby('itemid')['subject_id'].nunique().reset_index()\n","itemid_counts_df.columns = ['itemid', 'count']\n","itemid_counts_df['itemid'] = itemid_counts_df['itemid'].astype(str)\n","\n","\n","merged_df = pd.merge(itemid_counts_df, ad_test_names, on='itemid', how='inner')\n","\n","merged_df_sorted = merged_df.sort_values(by='count', ascending=False)\n","\n","merged_df_sorted"]},{"cell_type":"markdown","metadata":{"id":"MbG2nL3MKH3q"},"source":["### **Apply Word2vec embedding, treating each patient's test history as a \"sentence\" and each labtest as a \"word\".**"]},{"cell_type":"markdown","metadata":{"id":"1w-3D1b_KcM_"},"source":["Prepare the sentences:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pxtv-jjetD8","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":10,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["query = \"\"\"\n","  SELECT\n","    subject_id,\n","    STRING_AGG(CAST(itemid AS STRING) ORDER BY charttime ASC) AS itemid_sequence,  -- ASC for time order\n","    ARRAY_AGG(CAST(valuenum AS FLOAT64) ORDER BY charttime ASC) AS test_value_sequence,  -- ASC for time order\n","    ARRAY_LENGTH(ARRAY_AGG(itemid ORDER BY charttime ASC)) AS sequence_length,\n","    STRING_AGG(DISTINCT icd_code ORDER BY icd_code ASC) AS icd_codes,\n","    'mimic_iv' AS data_source,  -- New column with constant value 'mimic_iv'\n","    CASE\n","        WHEN REGEXP_CONTAINS(STRING_AGG(DISTINCT filtered.icd_code ORDER BY filtered.icd_code ASC), r'G30.*')\n","        OR REGEXP_CONTAINS(STRING_AGG(DISTINCT filtered.icd_code ORDER BY filtered.icd_code ASC), r'331.*')\n","        THEN 1\n","        ELSE 0\n","    END AS label_ad  -- New column that marks Alzheimer's Disease based on G30 ICD codes\n","  FROM (\n","    SELECT\n","        d.subject_id,\n","        l.itemid,\n","        l.valuenum,\n","        l.charttime,\n","        d.icd_code,\n","        ROW_NUMBER() OVER (PARTITION BY d.subject_id, l.itemid ORDER BY l.charttime ASC) AS rn\n","    FROM\n","        physionet-data.mimiciv_hosp.diagnoses_icd AS d\n","    JOIN\n","        physionet-data.mimiciv_hosp.labevents AS l\n","    ON\n","        d.subject_id = l.subject_id\n","    JOIN\n","        physionet-data.mimiciv_hosp.admissions AS a\n","    ON\n","        l.subject_id = a.subject_id\n","    JOIN\n","      physionet-data.mimiciv_hosp.d_labitems AS dlab\n","    ON\n","      l.itemid = dlab.itemid\n","    WHERE\n","      (\n","        d.icd_code LIKE 'G30%' OR\n","        d.icd_code LIKE 'F01%' OR\n","        d.icd_code LIKE 'F03%' OR\n","        d.icd_code LIKE 'F02%' OR\n","        d.icd_code LIKE 'R54%' OR\n","\n","        d.icd_code IN ('G318', 'G310', 'G311', 'G318', 'G319', '3310', '3311', '3312', '3319', '2904', '2900', '2901', '2902', '2903', '2908', '2909', '797')\n","      )\n","      AND l.valuenum IS NOT NULL\n","      # AND l.charttime <= a.dischtime\n","      AND l.hadm_id < a.hadm_id  -- Only include hadm_id before first AD diagnosis\n","      AND (LOWER(l.flag) LIKE 'abnormal%' OR l.flag IS NULL)\n","      # AND (LOWER(l.flag) LIKE 'abnormal%')\n","      AND lower(dlab.fluid) LIKE '%blood%'\n","\n","  ) AS filtered\n","  WHERE\n","    rn <= 2  -- only keep the first two entries for each itemid per patient\n","  GROUP BY\n","    subject_id\n","  ORDER BY\n","    subject_id;\n","\"\"\"\n","\n","\n","\n","\n","df_itemid_sequences_4 = bf.read_gbq(query)\n","print(len(df_itemid_sequences_4))\n","df_itemid_sequences_4.head(10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkGLbJj3rNFZ","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# prompt: plot distribution of df_itemid_sequences['sequence_length']\n","df_labtest_sequences_pd_4 = df_itemid_sequences_4.to_pandas()\n","sns.displot(df_labtest_sequences_pd_4['sequence_length'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3Q-tFk0G3ie","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["print(df_labtest_sequences_pd_4['sequence_length'].max())\n","print(df_labtest_sequences_pd_4['sequence_length'].min())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWZ8poPElPoX","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["print(df_labtest_sequences_pd_4.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"Y0ALfTLmAPvT"},"source":["### **Similarly for MIMIC_III**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH0f0javAVNb","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["query = \"\"\"\n","  SELECT\n","    subject_id,\n","    STRING_AGG(CAST(itemid AS STRING) ORDER BY charttime ASC) AS itemid_sequence,  -- ASC for time order\n","    ARRAY_AGG(CAST(valuenum AS FLOAT64) ORDER BY charttime ASC) AS test_value_sequence,  -- ASC for time order\n","    ARRAY_LENGTH(ARRAY_AGG(itemid ORDER BY charttime ASC)) AS sequence_length,\n","    STRING_AGG(DISTINCT icd9_code ORDER BY icd9_code ASC) AS icd_codes,\n","    'mimic_iii' AS data_source,\n","    CASE WHEN STRING_AGG(DISTINCT icd9_code ORDER BY icd9_code ASC) LIKE '331%' THEN 1 ELSE 0 END AS label_ad\n","  FROM (\n","    SELECT\n","      d.subject_id,\n","      l.itemid,\n","      l.valuenum,\n","      l.charttime,\n","      d.icd9_code,\n","      ROW_NUMBER() OVER (PARTITION BY d.subject_id, l.itemid ORDER BY l.charttime ASC) AS rn\n","    FROM\n","        `physionet-data.mimiciii_clinical.diagnoses_icd` AS d\n","    JOIN\n","        `physionet-data.mimiciii_clinical.labevents` AS l\n","    ON\n","        d.subject_id = l.subject_id\n","    JOIN\n","        `physionet-data.mimiciii_clinical.admissions` AS a\n","    ON\n","        l.subject_id = a.subject_id\n","    INNER JOIN\n","      `physionet-data.mimiciii_clinical.d_labitems` AS dlab\n","    ON\n","      l.itemid = dlab.itemid\n","    WHERE\n","      d.icd9_code LIKE '331%'  -- ICD-9 codes for Alzheimer's and related diseases\n","      AND l.valuenum IS NOT NULL\n","      # AND l.charttime <= a.dischtime\n","      AND l.hadm_id < a.hadm_id  -- Only include hadm_id before first AD diagnosis\n","      AND (LOWER(l.flag) LIKE 'abnormal%' OR l.flag IS NULL)\n","      # AND (LOWER(l.flag) LIKE 'abnormal%')\n","      AND lower(dlab.fluid) LIKE '%blood%'\n","\n","\n","  ) AS filtered\n","  WHERE\n","    rn <= 2  -- only keep the first two entries for each itemid per patient\n","  GROUP BY\n","    subject_id\n","  ORDER BY\n","    subject_id;\n","\"\"\"\n","df_itemid_sequences_3 = bf.read_gbq(query)\n","print(len(df_itemid_sequences_3))\n","df_itemid_sequences_3.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-53hElVWY-NI","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["df_labtest_sequences_pd_3 = df_itemid_sequences_3.to_pandas()\n","sns.displot(df_labtest_sequences_pd_3['sequence_length'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTK9PJUeITz5","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["df_labtest_sequences_pd_3.head()"]},{"cell_type":"markdown","metadata":{"id":"jQ-UD5_rzLmT"},"source":["**Combine data fromo mimic iii and iv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiFGRXVRzLAw","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# df_labtest_sequences_pd = df_itemid_sequences.to_pandas()\n","df_labtest_sequences_pd = pd.concat([df_labtest_sequences_pd_4, df_labtest_sequences_pd_3], ignore_index=True)\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","plt.rcParams.update({'font.size': 8})\n","\n","sns.displot(df_labtest_sequences_pd['sequence_length'], kde=True, color='#00BFFF')\n","\n","skewness = df_labtest_sequences_pd['sequence_length'].skew()\n","print(f'Skewness: {skewness}')\n","\n","plt.title(f'Sequence Length Distribution with Skewness = {skewness:.2f}')\n","plt.xlabel('Sequence Length')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEwuUORJ2_qi","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["for i in range(len(df_labtest_sequences_pd)):\n","  seq = df_labtest_sequences_pd.loc[i, 'itemid_sequence']\n","  if '52285' in seq:\n","    print(seq)"]},{"cell_type":"markdown","metadata":{"id":"1oVwLhgfSDJp"},"source":["**Split itemid sequences, save dataframe**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8robhc6SAKa","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["import ast\n","\n","\n","df_labtest_sequences_pd['itemid_sequence'] = df_labtest_sequences_pd['itemid_sequence'].apply(lambda x: x.split(',') if isinstance(x, str) else [])\n","# df_labtest_sequences_pd['test_value_sequence'] = df_labtest_sequences_pd['test_value_sequence'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n","# df_labtest_sequences_pd.to_csv(file_path + 'df_labtest_sequences_pd.csv', index=False)\n","\n","# patients_itemid_seqs = df_labtest_sequences_pd['itemid_sequence']\n","# patients_itemid_seqs[:1]\n","df_labtest_sequences_pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mis4Rv4nzoNQ","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# df_labtest_sequences_pd = pd.read_csv('df_labtest_sequences_pd.csv')\n","print(len(df_labtest_sequences_pd))\n","df_labtest_sequences_pd.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqJq6fJx445S","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["df_labtest_sequences_pd.dtypes"]},{"cell_type":"markdown","metadata":{"id":"Ao8IpKxPGkLY"},"source":["# **Apply scalers for each labtest**"]},{"cell_type":"markdown","metadata":{"id":"beVRco7n7zAD"},"source":["**Train scalers for each labtest itemid based on global data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaWHKvfr7x27","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# from sklearn.preprocessing import RobustScaler\n","\n","# labtest_scalers = {}\n","# outlier_ranges = []\n","\n","# unique_itemids = df_ad_patients_lab_results_pd['itemid'].unique()\n","# num_itemids = len(unique_itemids)\n","\n","# for i in range(num_itemids):\n","#   itemid = unique_itemids[i]\n","\n","#   item_values = df_ad_patients_lab_results_pd[df_ad_patients_lab_results_pd['itemid'] == itemid]['valuenum'].dropna()\n","#   # print(f\"Processing Itemid: {itemid}, values: {item_values}\")\n","\n","#   if item_values.empty:\n","#     print(f\"Itemid: {itemid} has no valid data, skipping.\")\n","#     continue\n","\n","\n","#   if item_values.size == 0:\n","#     print(f\"Itemid: {itemid} has no valid data, skipping.\")\n","#     continue\n","\n","#   sys.stdout.write(f\"\\rProcessing itemid {itemid} , {i + 1}/{num_itemids} ({(i + 1) / num_itemids * 100:.2f}%)\")\n","#   sys.stdout.flush()\n","\n","#   item_values_array = item_values.values.reshape(-1, 1)\n","\n","\n","#   scaler = RobustScaler()\n","#   scaler.fit(item_values_array)\n","\n","#   labtest_scalers[itemid] = scaler\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97PZOMABOu-N","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# print(len(labtest_scalers))\n","# print(labtest_scalers)"]},{"cell_type":"markdown","metadata":{"id":"MXaseeEzHomi"},"source":["**save scaler**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tvXr_VT_uw_","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# import pickle\n","\n","# with open(file_path + 'labtest_scalers.pkl', 'wb') as file:\n","#     pickle.dump(labtest_scalers, file)"]},{"cell_type":"markdown","metadata":{"id":"SPminjm0lMJ9"},"source":["**Concatenate vectors of patients to matrices**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"soTQ7CGMlWR6","executionInfo":{"status":"aborted","timestamp":1730451786617,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from collections import defaultdict\n","import sys\n","\n","df_all = copy.deepcopy(df_labtest_sequences_pd)\n","\n","# df_all['itemid_sequence'] = df_all['itemid_sequence'].apply(lambda x: list(x) if isinstance(x, list) else x)\n","df_all['test_value_sequence'] = df_all['test_value_sequence'].apply(lambda x: list(x) if isinstance(x, list) else x)\n","\n","\n","df_all['matrix'] = None\n","df_all['scaled_value_sequence'] = None\n","\n","missing_scaler_count = defaultdict(int)\n","missing_pca_vector_count = defaultdict(int)\n","outlier_count = defaultdict(int)\n","\n","num_patients = len(df_all)\n","\n","for i in range(len(df_all)):\n","\n","  patient_matrix = []\n","  updated_itemid_sequence = []\n","  updated_test_value_sequence = []\n","  scaled_values = []\n","\n","  sys.stdout.write(f\"\\rProcessing patient {i + 1}/{num_patients} ({(i + 1) / num_patients * 100:.2f}%)\")\n","  sys.stdout.flush()\n","\n","  itemid_sequence = df_all.loc[i, 'itemid_sequence']\n","  test_value_sequence = df_all.loc[i, 'test_value_sequence']\n","  # print(itemid_sequence)\n","  # print(test_value_sequence)\n","\n","  # Iterate through itemid and test_value\n","  for itemid, test_value in zip(itemid_sequence, test_value_sequence):\n","    if itemid == '52285':\n","      print(itemid, test_value)\n","    #\n","    # print(itemid, test_value)\n","    # scaler = labtest_scalers.get(itemid)\n","    # if scaler is None:\n","    #     missing_scaler_count[itemid] += 1\n","    #     continue  # Skip if no scaler for the itemid\n","\n","    # item_range = df_outlier_ranges[df_outlier_ranges['itemid'] == itemid]\n","    # # print(item_range)\n","    # lower_bound = item_range['lower_bound'].values[0] if not item_range.empty else None\n","    # upper_bound = item_range['upper_bound'].values[0] if not item_range.empty else None\n","\n","    # if lower_bound is not None and upper_bound is not None:\n","    #     if test_value < lower_bound or test_value > upper_bound:\n","    #         outlier_count[itemid] += 1\n","    #         continue  # Skip if the test value is outside the outlier range\n","    # else:\n","    #     print(f\"No outlier range found for itemid: {itemid}\")\n","    #     continue\n","\n","\n","    pca_vector = itemid_to_vector.get(str(itemid))\n","    if pca_vector is None:\n","        missing_pca_vector_count[itemid] += 1\n","        continue  # Skip if no PCA vector for the itemid\n","\n","    # Combine the 20-dim PCA vector with the test value to form a 21-dim vector\n","    test_value_array = np.array(test_value).reshape(-1, 1)\n","\n","    # test_value_scaled = scaler.transform(test_value_array)\n","    # print(test_value, test_value_scaled)\n","\n","    # combined_vector = np.append(pca_vector, test_value_scaled)\n","    combined_vector = np.append(pca_vector, test_value)\n","\n","    patient_matrix.append(combined_vector)\n","    # print(combined_vector)\n","\n","    # Update sequences\n","    updated_itemid_sequence.append(itemid)\n","    updated_test_value_sequence.append(test_value)\n","    # scaled_values.append(test_value_scaled.item())\n","\n","  # Update the patient's matrix and sequences in the DataFrame\n","  df_all.at[i, 'matrix'] = patient_matrix\n","  df_all.at[i, 'itemid_sequence'] = updated_itemid_sequence\n","  df_all.at[i, 'test_value_sequence'] = updated_test_value_sequence\n","  df_all.at[i, 'sequence_length'] = len(updated_itemid_sequence)\n","  # df_all.at[i, 'scaled_value_sequence'] = scaled_values\n","\n","print('\\n')\n","print(df_all.head(1))\n","print(f\"Number of patients: {len(df_all)}\")\n","# print(f\"Number of values with missing scaler: {missing_scaler_count}\")\n","print(f\"Number of values with missing pca vector: {missing_pca_vector_count}\")\n","print(f\"Number of values outside outlier range: {outlier_count}\")\n","\n","df_all = df_all[df_all['sequence_length'] >= 4].reset_index(drop=True)\n","df_all.head(5)\n"]},{"cell_type":"markdown","metadata":{"id":"GlFI1Z3TJCz7"},"source":["# **save matrix df** (not fixed yet) **df_labtest_sequences_pd_pre_blood.csv**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpe82xLwH-Vn","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["# df_all['matrix'] = df_all['matrix'].apply(lambda x: str(x))\n","# df_all.to_csv(file_path + 'df_labtest_sequences_pd_pre_blood.csv', index=False)\n","# np.save(file_path + 'matrix_data_pre_blood.npy', df_all['matrix'].values)"]},{"cell_type":"code","source":["# df_all = pd.read_csv(file_path + 'df_labtest_sequences_pd_pre_blood.csv')\n","# df_all['matrix'] = df_all['matrix'].apply(lambda x: np.array(eval(x, {'array': np.array})))\n"],"metadata":{"id":"SyuZlGUUjhdO","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szMI_SfHy7oz","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["\n","# df_labtest_sequences_pd = copy.deepcopy(df_all)\n","# df_all.head(1)"]},{"cell_type":"markdown","source":["# **GRU**"],"metadata":{"id":"xOnkYpSz66-k"}},{"cell_type":"code","source":["df_labtest_GRU = copy.deepcopy(df_all)\n","df_labtest_GRU.head()"],"metadata":{"id":"iIZPL9Wk_iRF","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","import copy\n","import random\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    roc_curve,\n","    auc\n",")\n","import matplotlib.pyplot as plt\n","\n","class EarlyStopping:\n","    def __init__(self, patience=7):\n","        self.patience = patience\n","        self.counter = 0\n","        self.best_score = None\n","        self.best_model = None\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss, model):\n","        score = -val_loss\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.best_model = copy.deepcopy(model.state_dict())\n","        elif score < self.best_score:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.best_model = copy.deepcopy(model.state_dict())\n","            self.counter = 0\n","\n","class BalancedFocalLoss(nn.Module):\n","    def __init__(self, alpha=0.5, gamma=2):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        batch_size = targets.size(0)\n","        class_weights = torch.bincount(targets).float()\n","        class_weights = batch_size / (2 * class_weights)\n","        sample_weights = class_weights[targets]\n","        focal_loss = sample_weights * self.alpha * (1-pt)**self.gamma * ce_loss\n","        return focal_loss.mean()\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, hidden_size):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.LayerNorm(hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_size, hidden_size)\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","class EnhancedMedicalGRUDelta(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.feature_extraction = nn.Sequential(\n","            nn.Linear(input_size, hidden_size * 2),\n","            nn.LayerNorm(hidden_size * 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_prob),\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            ResidualBlock(hidden_size)\n","        )\n","\n","        self.gru = nn.GRU(\n","            input_size=hidden_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout_prob,\n","            bidirectional=True\n","        )\n","\n","        self.attention = nn.MultiheadAttention(\n","            embed_dim=hidden_size * 2,\n","            num_heads=16,\n","            dropout=dropout_prob\n","        )\n","\n","        self.residual_conn = nn.Linear(input_size, hidden_size * 2)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.LayerNorm(hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_prob),\n","            nn.Linear(hidden_size, 2)\n","        )\n","\n","    def forward(self, x):\n","        x_lengths = [len(seq) for seq in x]\n","        x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True)\n","        batch_size = x_padded.size(0)\n","\n","        x_reshaped = x_padded.view(-1, x_padded.size(-1))\n","        x_features = self.feature_extraction(x_reshaped)\n","        x_features = x_features.view(batch_size, -1, self.hidden_size)\n","\n","        packed_input = nn.utils.rnn.pack_padded_sequence(\n","            x_features,\n","            x_lengths,\n","            batch_first=True,\n","            enforce_sorted=False\n","        )\n","\n","        packed_output, _ = self.gru(packed_input)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","\n","        residual = self.residual_conn(x_padded)\n","\n","        attention_output, _ = self.attention(\n","            output.transpose(0, 1),\n","            output.transpose(0, 1),\n","            output.transpose(0, 1)\n","        )\n","        attention_output = attention_output.transpose(0, 1)\n","        attention_output = attention_output + residual\n","\n","        mask = torch.arange(output.size(1))[None, :] < torch.tensor(x_lengths)[:, None]\n","        mask = mask.to(output.device)\n","        masked_output = attention_output * mask.unsqueeze(-1)\n","        pooled = masked_output.sum(dim=1) / mask.sum(dim=1, keepdim=True)\n","\n","        return self.classifier(pooled)\n","\n","def prepare_medical_data(df):\n","    train_df, eval_df = train_test_split(\n","        df,\n","        test_size=0.1,\n","        random_state=42,\n","        stratify=df['label_ad']\n","    )\n","\n","    true_eval_df = eval_df[eval_df['label_ad'] == 1]\n","    false_eval_df = eval_df[eval_df['label_ad'] == 0]\n","    downsampled_false_eval_df = false_eval_df.sample(n=len(true_eval_df), random_state=42)\n","    eval_df = pd.concat([true_eval_df, downsampled_false_eval_df])\n","\n","    def process_sequence(matrix):\n","        if not isinstance(matrix, np.ndarray):\n","            matrix = np.array(matrix)\n","        return torch.tensor(matrix, dtype=torch.float32)\n","\n","    X_train_tensor = [process_sequence(row['matrix']) for _, row in train_df.iterrows()]\n","    y_train_tensor = torch.tensor(train_df['label_ad'].values, dtype=torch.long)\n","\n","    X_eval_tensor = [process_sequence(row['matrix']) for _, row in eval_df.iterrows()]\n","    y_eval_tensor = torch.tensor(eval_df['label_ad'].values, dtype=torch.long)\n","\n","    return X_train_tensor, y_train_tensor, X_eval_tensor, y_eval_tensor\n","\n","def train_model(model, X_train, y_train, X_eval, y_eval, config):\n","    criterion = BalancedFocalLoss()\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=config['learning_rate'],\n","        weight_decay=config['weight_decay']\n","    )\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='min',\n","        factor=0.5,\n","        patience=3,\n","        min_lr=1e-6\n","    )\n","    early_stopping = EarlyStopping(patience=config['patience'])\n","\n","    train_losses, eval_losses = [], []\n","    train_accs, eval_accs = [], []\n","\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        permutation = torch.randperm(len(X_train))\n","        for i in range(0, len(X_train), config['batch_size']):\n","            optimizer.zero_grad()\n","            indices = permutation[i:i + config['batch_size']]\n","            batch_X = [X_train[idx] for idx in indices]\n","            batch_y = y_train[indices]\n","\n","            outputs = model(batch_X)\n","            loss = criterion(outputs, batch_y)\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += batch_y.size(0)\n","            correct += predicted.eq(batch_y).sum().item()\n","\n","        train_loss = train_loss / (len(X_train) // config['batch_size'])\n","        train_acc = correct / total\n","\n","        model.eval()\n","        with torch.no_grad():\n","            eval_outputs = model(X_eval)\n","            eval_loss = criterion(eval_outputs, y_eval).item()\n","            _, predicted = eval_outputs.max(1)\n","            eval_acc = predicted.eq(y_eval).sum().item() / len(y_eval)\n","\n","        train_losses.append(train_loss)\n","        eval_losses.append(eval_loss)\n","        train_accs.append(train_acc)\n","        eval_accs.append(eval_acc)\n","\n","        current_lr = optimizer.param_groups[0]['lr']\n","        print(f'Epoch {epoch+1}/{config[\"num_epochs\"]} - '\n","              f'Train Loss: {train_loss:.4f}, '\n","              f'Train Acc: {train_acc:.4f}, '\n","              f'Val Loss: {eval_loss:.4f}, '\n","              f'Val Acc: {eval_acc:.4f}, '\n","              f'LR: {current_lr:.6f}')\n","\n","        scheduler.step(eval_loss)\n","        if early_stopping(eval_loss, model):\n","            print(\"Early stopping triggered\")\n","            model.load_state_dict(early_stopping.best_model)\n","            break\n","\n","    return model, train_losses, eval_losses, train_accs, eval_accs\n","\n","config = {\n","    'hidden_size': 256,\n","    'num_layers': 3,\n","    'dropout_prob': 0.5,\n","    'learning_rate': 0.001,\n","    'weight_decay': 0.01,\n","    'batch_size': 32,\n","    'num_epochs': 10,\n","    'patience': 5\n","}\n","\n","X_train, y_train, X_eval, y_eval = prepare_medical_data(df_labtest_GRU)\n","config['input_size'] = X_train[0].shape[1]\n","\n","model = EnhancedMedicalGRUDelta(**{k: v for k, v in config.items()\n","                                  if k in ['input_size', 'hidden_size',\n","                                         'num_layers', 'dropout_prob']})\n","\n","model, train_losses, eval_losses, train_accs, eval_accs = train_model(\n","    model, X_train, y_train, X_eval, y_eval, config\n",")\n","\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(eval_losses, label='Val Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accs, label='Train Acc')\n","plt.plot(eval_accs, label='Val Acc')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_eval)\n","    _, predicted = torch.max(outputs, 1)\n","\n","    accuracy = (predicted == y_eval).sum().item() / len(y_eval)\n","    print(f\"Final Accuracy: {accuracy:.4f}\")\n","    print(f\"Confusion Matrix:\\n{confusion_matrix(y_eval, predicted)}\")\n","    print(f\"Precision: {precision_score(y_eval, predicted):.4f}\")\n","    print(f\"Recall: {recall_score(y_eval, predicted):.4f}\")\n","    print(f\"F1 Score: {f1_score(y_eval, predicted):.4f}\")\n","\n","    probs = F.softmax(outputs, dim=1)[:, 1]\n","    fpr, tpr, _ = roc_curve(y_eval, probs)\n","    roc_auc = auc(fpr, tpr)\n","    print(f\"AUC: {roc_auc:.4f}\")\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"l6a9lBNm_ibG","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6nsCG0nlEQi"},"source":["# **Apply LSTM**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0_Ef-86rYo2v"},"source":[]},{"cell_type":"markdown","metadata":{"id":"SPR5nURWOjQ3"},"source":[]},{"cell_type":"markdown","metadata":{"id":"z2_NB-V9YuF1"},"source":["\n","\n"]},{"cell_type":"code","source":["random_seed = 42\n","df_labtest_sequences_pd = copy.deepcopy(df_all)\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn.utils.rnn as rnn_utils\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n","import os\n","\n","# Define a directory to save the model checkpoints\n","checkpoint_dir = file_path + '/model_checkpoints/pre_blood'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# True for label_ad == 1\n","df_labtest_sequences_pd['label'] = df_labtest_sequences_pd['label_ad'].apply(lambda x: True if x == 1 else False)\n","\n","# data distribution\n","print(\"Initial Distribution of Labels (True for Alzheimer's, False for Other):\")\n","print(df_labtest_sequences_pd['label'].value_counts())\n","\n","# split dataset\n","train_df, eval_df = train_test_split(df_labtest_sequences_pd, test_size=0.1, random_state=random_seed, stratify=df_labtest_sequences_pd['label'])\n","\n","# resample True\n","true_train_df = train_df[train_df['label'] == True]\n","duplicated_true_train_df = pd.concat([true_train_df] * 1)  # change number for re-sampling\n","\n","# downsample False\n","false_train_df = train_df[train_df['label'] == False]\n","\n","filtered_false_train_df = false_train_df[\n","    (false_train_df['sequence_length'] >= 4)\n","]\n","\n","downsampled_false_train_df = filtered_false_train_df.sample(frac=0.7, random_state=random_seed)\n","\n","# combine\n","train_df = pd.concat([duplicated_true_train_df, downsampled_false_train_df])\n","\n","print(\"Resampled Distribution of Labels (True for Alzheimer's, False for Other):\")\n","print(train_df['label'].value_counts())\n","\n","# matrix to PyTorch tensors\n","X_train_tensor = [torch.tensor(matrix, dtype=torch.float32) for matrix in train_df['matrix']]\n","y_train_tensor = torch.tensor(train_df['label'].astype(int).values, dtype=torch.long)\n","\n","# eval dataset\n","true_eval_df = eval_df[eval_df['label'] == True]\n","false_eval_df = eval_df[eval_df['label'] == False]\n","\n","downsampled_false_eval_df = false_eval_df.sample(n=len(true_eval_df), random_state=random_seed)\n","\n","eval_df = pd.concat([true_eval_df, downsampled_false_eval_df])\n","\n","X_eval_tensor = [torch.tensor(matrix, dtype=torch.float32) for matrix in eval_df['matrix']]\n","y_eval_tensor = torch.tensor(eval_df['label'].astype(int).values, dtype=torch.long)\n","\n","print(\"Resampled Evaluation Distribution of Labels (True for Alzheimer's, False for Other):\")\n","print(eval_df['label'].value_counts())\n"],"metadata":{"id":"0COnW3Ea0nhx","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdMrYpYpJIxL","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":9,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["\n","# model hyperparams\n","input_size = X_train_tensor[0].shape[1]  # 21 dim\n","hidden_size = 32\n","num_layers = 2\n","\n","class AlzSeqLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout_prob=0.5):\n","        super(AlzSeqLSTM, self).__init__()\n","        # LSTM layer\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","        # Hidden layer\n","        self.hidden_layer = nn.Linear(hidden_size, hidden_size)  # A hidden layer before FC\n","        self.relu = nn.ReLU()  # Activation function for the hidden layer\n","\n","        # Dropout layer\n","        self.dropout = nn.Dropout(dropout_prob)  # Dropout with a probability of dropout_prob\n","\n","        # Fully connected layer for classification\n","        self.fc = nn.Linear(hidden_size, 2)  # Output layer for binary classification\n","\n","    def forward(self, x):\n","        packed_input = rnn_utils.pack_sequence(x, enforce_sorted=False)\n","        packed_output, (h_n, c_n) = self.lstm(packed_input)\n","        lstm_out, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n","\n","        # Hidden layer followed by ReLU and Dropout\n","        hidden_out = self.hidden_layer(h_n[-1])\n","        hidden_out = self.relu(hidden_out)\n","        hidden_out = self.dropout(hidden_out)  # Apply dropout after ReLU\n","\n","        # Fully connected layer\n","        out = self.fc(hidden_out)\n","        return out\n","\n","\n","# init model\n","model = AlzSeqLSTM(input_size, hidden_size, num_layers)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n","\n","# train\n","num_epochs = 50\n","batch_size = 16\n","train_losses = []\n","eval_losses = []\n","train_accs = []\n","train_f1s = []\n","eval_accs = []\n","eval_f1s = []\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","  permutation = torch.randperm(len(X_train_tensor))\n","  epoch_loss = 0.0\n","  correct_pred = 0\n","  total_pred = 0\n","\n","  for i in range(0, len(X_train_tensor), batch_size):\n","    optimizer.zero_grad()\n","\n","    batch_indices = permutation[i:i + batch_size]\n","    batch_X = [X_train_tensor[idx] for idx in batch_indices]\n","    batch_y = y_train_tensor[batch_indices]\n","\n","    # feed forward\n","    outputs = model(batch_X)\n","    loss = criterion(outputs, batch_y)\n","    _, predicted = torch.max(outputs, 1)\n","    total_pred += batch_y.size(0)\n","    correct_pred += (predicted == batch_y).sum().item()\n","\n","\n","    # back prop\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  train_loss = epoch_loss / int(len(X_train_tensor)/batch_size)\n","  train_losses.append(train_loss)\n","  train_acc = correct_pred / total_pred\n","  train_accs.append(train_acc)\n","\n","  # validation loss\n","  model.eval()\n","  with torch.no_grad():\n","    eval_outputs = model(X_eval_tensor)\n","    # print(eval_outputs)\n","    # print(y_eval_tensor)\n","\n","    eval_loss = criterion(eval_outputs, y_eval_tensor).item()\n","    _, eval_predicted = torch.max(eval_outputs, 1)\n","    eval_acc = (eval_predicted == y_eval_tensor).sum().item() / len(y_eval_tensor)\n","    eval_losses.append(eval_loss)\n","    eval_accs.append(eval_acc)\n","\n","  print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}, Train Acc: {train_acc:.4f}, Eval Acc: {eval_acc:.4f}\")\n","\n","  # Save the model checkpoint\n","  checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch + 1}.pth')\n","  torch.save({\n","      'epoch': epoch + 1,\n","      'model_state_dict': model.state_dict(),\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      'train_loss': train_loss,\n","      'eval_loss': eval_loss,\n","      'train_acc': train_acc,\n","      'eval_acc': eval_acc\n","  }, checkpoint_path)\n","\n","# train and valid losses\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(eval_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Train and Validation Loss')\n","plt.legend()\n","\n","# accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accs, label='Train Accuracy')\n","plt.plot(eval_accs, label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Train and Validation Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","source":["\n","# Function to load a specific model checkpoint\n","def load_model_checkpoint(checkpoint_path, model, optimizer):\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    train_loss = checkpoint['train_loss']\n","    eval_loss = checkpoint['eval_loss']\n","    train_acc = checkpoint['train_acc']\n","    eval_acc = checkpoint['eval_acc']\n","\n","    print(f\"Loaded model from epoch {epoch}: Train Loss = {train_loss:.4f}, Eval Loss = {eval_loss:.4f}, Train Acc = {train_acc:.4f}, Eval Acc = {eval_acc:.4f}\")\n","    return model, optimizer\n","\n","# Change number for best epoch\n","usedmodel, optimizer = load_model_checkpoint(checkpoint_dir + '/epoch_20.pth', model, optimizer)\n"],"metadata":{"id":"TfeO2vqHHRMG","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0HOLOlQJQgp","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n","\n","# evaluate\n","usedmodel.eval()\n","with torch.no_grad():\n","  outputs = usedmodel(X_eval_tensor)\n","  _, predicted = torch.max(outputs, 1)\n","  accuracy = (predicted == y_eval_tensor).sum().item() / len(y_eval_tensor)\n","  print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n","\n","  # confusion matrix\n","  cm = confusion_matrix(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy())\n","  print(f\"Confusion Matrix:\\n{cm}\")\n","\n","  # Precision, Recall, F1 Score\n","  precision = precision_score(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy(), pos_label=1)\n","  recall = recall_score(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy(), pos_label=1)\n","  f1 = f1_score(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy(), pos_label=1)\n","\n","  print(f\"Precision: {precision:.4f}\")\n","  print(f\"Recall: {recall:.4f}\")\n","  print(f\"F1 Score: {f1:.4f}\")\n","\n","  # ROC and AUC calculation\n","  y_proba = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # probabilities for class 1\n","  fpr, tpr, _ = roc_curve(y_eval_tensor.cpu().numpy(), y_proba)\n","  roc_auc = auc(fpr, tpr)\n","\n","  # Plot ROC curve\n","  plt.figure()\n","  plt.rcParams.update({'font.size': 8})\n","\n","  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('Receiver Operating Characteristic (ROC)')\n","  plt.legend(loc=\"lower right\")\n","  plt.show()\n","\n","  # np.savez(file_path + 'results_pre_blood_best_roc.npz', fpr=fpr, tpr=tpr, roc_auc=roc_auc)"]},{"cell_type":"code","source":["\n","import torch.nn.utils.rnn as rnn_utils\n","\n","# Define Transformer-based model without an embedding layer\n","class AlzSeqTransformer(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout_prob=0.5):\n","        super(AlzSeqTransformer, self).__init__()\n","\n","        # Transformer Encoder Layer\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dropout=dropout_prob)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","        # Classification head\n","        self.fc = nn.Linear(input_size, 2)  # Assuming binary classification\n","\n","    def forward(self, x):\n","        # Pad sequences to the same length\n","        x_padded = rnn_utils.pad_sequence(x, batch_first=True)\n","\n","        # Transformer encoder\n","        transformer_out = self.transformer_encoder(x_padded.transpose(0, 1))  # Transpose for Transformer input\n","\n","        # Mean pooling over the sequence length dimension\n","        transformer_out = transformer_out.mean(dim=0)  # Mean pooling over time steps\n","        out = self.fc(transformer_out)\n","        return out\n","\n","# Initialize model parameters\n","input_size = X_train_tensor[0].shape[1]  # Embedding dimension\n","hidden_size = 32\n","num_layers = 2\n","num_heads = 3\n","\n","# Initialize model, criterion, optimizer\n","model = AlzSeqTransformer(input_size, hidden_size, num_layers, num_heads)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    permutation = torch.randperm(len(X_train_tensor))\n","    epoch_loss = 0.0\n","    correct_pred = 0\n","    total_pred = 0\n","\n","    for i in range(0, len(X_train_tensor), batch_size):\n","        optimizer.zero_grad()\n","        batch_indices = permutation[i:i + batch_size]\n","        batch_X = [X_train_tensor[idx] for idx in batch_indices]\n","        batch_y = y_train_tensor[batch_indices]\n","\n","        # Forward pass\n","        outputs = model(batch_X)\n","        loss = criterion(outputs, batch_y)\n","\n","        _, predicted = torch.max(outputs, 1)\n","        total_pred += batch_y.size(0)\n","        correct_pred += (predicted == batch_y).sum().item()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    # Calculate and store training metrics\n","    train_loss = epoch_loss / int(len(X_train_tensor) / batch_size)\n","    train_losses.append(train_loss)\n","    train_acc = correct_pred / total_pred\n","    train_accs.append(train_acc)\n","\n","    # Evaluation phase\n","    model.eval()\n","    with torch.no_grad():\n","        eval_outputs = model(X_eval_tensor)\n","        eval_loss = criterion(eval_outputs, y_eval_tensor).item()\n","        _, eval_predicted = torch.max(eval_outputs, 1)\n","        eval_acc = (eval_predicted == y_eval_tensor).sum().item() / len(y_eval_tensor)\n","        eval_losses.append(eval_loss)\n","        eval_accs.append(eval_acc)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}, Train Acc: {train_acc:.4f}, Eval Acc: {eval_acc:.4f}\")\n"],"metadata":{"id":"E583cYuRySUZ","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"kVXgGZ_j75DQ","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), file_path + 'model_pre_blood.pth')\n"],"metadata":{"id":"LITH0ROZzduU","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdHVGW05AUyh","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["for i in range(len(df_all)):\n","  seq = df_all.loc[i, 'itemid_sequence']\n","  if '52285' in seq:\n","    print('CSF')\n","  if '51105' in seq:\n","    print('Urine')\n","  # if '50911' in seq:\n","  #   print('Blood')"]},{"cell_type":"markdown","source":["## No need to run the following"],"metadata":{"id":"-Y_ax8qJvxGc"}},{"cell_type":"code","source":[],"metadata":{"id":"9oImuMcrvwoA","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YVb3NYMPFr6","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["df_labtest_sequences_pd = copy.deepcopy(df_all)\n","df_labtest_sequences_pd['label'] = df_labtest_sequences_pd['label_ad'].apply(lambda x: True if x == 1 else False)\n","\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","\n","\n","k_folds = 10\n","kf_true = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)\n","kf_false = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)\n","\n","class AlzSeqLSTM(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers):\n","    super(AlzSeqLSTM, self).__init__()\n","    # LSTM layer\n","    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","    # Hidden layer\n","    self.hidden_layer = nn.Linear(hidden_size, hidden_size)  # A hidden layer before FC\n","    self.relu = nn.ReLU()  # Activation function for the hidden layer\n","    # Fully connected layer for classification\n","    self.fc = nn.Linear(hidden_size, 2)  # Output layer for binary classification\n","\n","  def forward(self, x):\n","    packed_input = rnn_utils.pack_sequence(x, enforce_sorted=False)\n","    packed_output, (h_n, c_n) = self.lstm(packed_input)\n","    lstm_out, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n","\n","    # hidden layer followed by ReLU\n","    hidden_out = self.hidden_layer(h_n[-1])\n","    hidden_out = self.relu(hidden_out)\n","\n","    # fully connected layer\n","    out = self.fc(hidden_out)\n","    return out\n","\n","train_losses_folds = []\n","eval_losses_folds = []\n","train_accs_folds = []\n","eval_accs_folds = []\n","roc_aucs_folds = []\n","\n","num_epochs = 50\n","batch_size = 32\n","input_size = X_train_tensor[0].shape[1]  # 21 dim\n","hidden_size = 32\n","num_layers = 2\n","\n","# k fold\n","true_data = df_labtest_sequences_pd[df_labtest_sequences_pd['label'] == True]\n","false_data = df_labtest_sequences_pd[df_labtest_sequences_pd['label'] == False]\n","fold = 0\n","for random_seed in [1, 6, 12, 18, 24, 30, 36, 42]:\n","  print(f'Fold {fold + 1}/{k_folds}')\n","  fold += 1\n","\n","  # split dataset\n","  train_df, eval_df = train_test_split(df_labtest_sequences_pd, test_size=0.1, random_state=random_seed, stratify=df_labtest_sequences_pd['label'])\n","\n","  # resample True\n","  true_train_df = train_df[train_df['label'] == True]\n","  duplicated_true_train_df = pd.concat([true_train_df] * 2)  # change number for re-sampling\n","\n","  # downsample False\n","  false_train_df = train_df[train_df['label'] == False]\n","\n","  filtered_false_train_df = false_train_df[\n","    (false_train_df['sequence_length'] >= 4)\n","  ]\n","\n","  downsampled_false_train_df = filtered_false_train_df.sample(frac=0.4, random_state=random_seed)\n","\n","  # combine\n","  train_df = pd.concat([duplicated_true_train_df, downsampled_false_train_df])\n","\n","  # print(\"Resampled Distribution of Labels (True for Alzheimer's, False for Other):\")\n","  # print(train_df['label'].value_counts())\n","\n","  # matrix to PyTorch tensors\n","  X_train_tensor = [torch.tensor(matrix, dtype=torch.float32) for matrix in train_df['matrix']]\n","  y_train_tensor = torch.tensor(train_df['label'].astype(int).values, dtype=torch.long)\n","\n","  # eval dataset\n","  true_eval_df = eval_df[eval_df['label'] == True]\n","  false_eval_df = eval_df[eval_df['label'] == False]\n","\n","  downsampled_false_eval_df = false_eval_df.sample(n=len(true_eval_df), random_state=random_seed)\n","\n","  eval_df = pd.concat([true_eval_df, downsampled_false_eval_df])\n","\n","  X_eval_tensor = [torch.tensor(matrix, dtype=torch.float32) for matrix in eval_df['matrix']]\n","  y_eval_tensor = torch.tensor(eval_df['label'].astype(int).values, dtype=torch.long)\n","\n","  # params\n","  model = AlzSeqLSTM(input_size, hidden_size, num_layers)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n","\n","  #\n","\n","  train_losses = []\n","  eval_losses = []\n","  train_accs = []\n","  eval_accs = []\n","\n","  for epoch in range(num_epochs):\n","    model.train()\n","    permutation = torch.randperm(len(X_train_tensor))\n","    epoch_loss = 0.0\n","    correct_pred = 0\n","    total_pred = 0\n","\n","    for i in range(0, len(X_train_tensor), batch_size):\n","      optimizer.zero_grad()\n","\n","      batch_indices = permutation[i:i + batch_size]\n","      batch_X = [X_train_tensor[idx] for idx in batch_indices]\n","      batch_y = y_train_tensor[batch_indices]\n","\n","      # feed forward\n","      outputs = model(batch_X)\n","      loss = criterion(outputs, batch_y)\n","      _, predicted = torch.max(outputs, 1)\n","      total_pred += batch_y.size(0)\n","      correct_pred += (predicted == batch_y).sum().item()\n","\n","\n","      # back prop\n","      loss.backward()\n","      optimizer.step()\n","\n","      epoch_loss += loss.item()\n","\n","    train_loss = epoch_loss / int(len(X_train_tensor)/batch_size)\n","    train_losses.append(train_loss)\n","    train_acc = correct_pred / total_pred\n","    train_accs.append(train_acc)\n","\n","    # validation loss\n","    model.eval()\n","    with torch.no_grad():\n","      eval_outputs = model(X_eval_tensor)\n","      # print(eval_outputs)\n","      # print(y_eval_tensor)\n","\n","      eval_loss = criterion(eval_outputs, y_eval_tensor).item()\n","      _, eval_predicted = torch.max(eval_outputs, 1)\n","      eval_acc = (eval_predicted == y_eval_tensor).sum().item() / len(y_eval_tensor)\n","      eval_losses.append(eval_loss)\n","      eval_accs.append(eval_acc)\n","\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}, Train Acc: {train_acc:.4f}, Eval Acc: {eval_acc:.4f}\")\n","\n","  #\n","  train_losses_folds.append(train_losses)\n","  eval_losses_folds.append(eval_losses)\n","  train_accs_folds.append(train_accs)\n","  eval_accs_folds.append(eval_accs)\n","\n","  # ROC AUC score\n","  with torch.no_grad():\n","    eval_outputs = model(X_eval_tensor)\n","    eval_probs = F.softmax(eval_outputs, dim=1)[:, 1].cpu().numpy()\n","    roc_auc = roc_auc_score(y_eval_tensor.cpu().numpy(), eval_probs)\n","    roc_aucs_folds.append(roc_auc)\n","\n","  print(f\"Fold {fold + 1}/{k_folds}: ROC AUC = {roc_auc:.4f}\")\n","\n","#\n","avg_train_losses = np.mean(train_losses_folds, axis=0)\n","avg_eval_losses = np.mean(eval_losses_folds, axis=0)\n","avg_train_accs = np.mean(train_accs_folds, axis=0)\n","avg_eval_accs = np.mean(eval_accs_folds, axis=0)\n","avg_roc_auc = np.mean(roc_aucs_folds)\n","\n","# loss\n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.plot(avg_train_losses, label='Average Train Loss')\n","plt.plot(avg_eval_losses, label='Average Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title(f'K-Fold Cross-Validation: Loss (Avg over {k_folds} Folds)')\n","plt.legend()\n","\n","# acc\n","plt.subplot(1, 2, 2)\n","plt.plot(avg_train_accs, label='Average Train Accuracy')\n","plt.plot(avg_eval_accs, label='Average Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title(f'K-Fold Cross-Validation: Accuracy (Avg over {k_folds} Folds)')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# RCO\n","fpr, tpr, _ = roc_curve(y_eval_tensor.cpu().numpy(), eval_probs)\n","plt.figure()\n","plt.plot(fpr, tpr, label=f'ROC curve (area = {avg_roc_auc:.2f})')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF1BgBlH_G1P","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["np.savez(file_path + 'results_pre_blood.npz', avg_train_losses=avg_train_losses, avg_eval_losses=avg_eval_losses, avg_train_accs=avg_train_accs, avg_eval_accs=avg_eval_accs, fpr=fpr, tpr=tpr, avg_roc_auc=avg_roc_auc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKBPP-OOSWSQ","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":8,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["random_seed = 42\n","df_labtest_sequences_pd = df_all.copy()\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn.utils.rnn as rnn_utils\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n","\n","\n","# True for label_ad == 1\n","df_labtest_sequences_pd['label'] = df_labtest_sequences_pd['label_ad'].apply(lambda x: True if x == 1 else False)\n","\n","# data distribution\n","print(\"Initial Distribution of Labels (True for Alzheimer's, False for Other):\")\n","print(df_labtest_sequences_pd['label'].value_counts())\n","\n","# split dataset\n","train_df, eval_df = train_test_split(df_labtest_sequences_pd, test_size=0.1, random_state=random_seed, stratify=df_labtest_sequences_pd['label'])\n","\n","# resample True\n","true_train_df = train_df[train_df['label'] == True]\n","duplicated_true_train_df = pd.concat([true_train_df] * 2)  # change number for re-sampling\n","\n","# downsample False\n","false_train_df = train_df[train_df['label'] == False]\n","\n","filtered_false_train_df = false_train_df[\n","    (false_train_df['sequence_length'] >= 4)\n","]\n","\n","downsampled_false_train_df = filtered_false_train_df.sample(frac=0.4, random_state=random_seed)\n","\n","# combine\n","train_df = pd.concat([duplicated_true_train_df, downsampled_false_train_df])\n","\n","print(\"Resampled Distribution of Labels (True for Alzheimer's, False for Other):\")\n","print(train_df['label'].value_counts())\n","\n","# matrix to PyTorch tensors\n","X_train_tensor = [torch.tensor(matrix, dtype=torch.float32) for matrix in train_df['matrix']]\n","y_train_tensor = torch.tensor(train_df['label'].astype(int).values, dtype=torch.long)\n","\n","# eval dataset\n","true_eval_df = eval_df[eval_df['label'] == True]\n","false_eval_df = eval_df[eval_df['label'] == False]\n","\n","downsampled_false_eval_df = false_eval_df.sample(n=len(true_eval_df), random_state=random_seed)\n","\n","eval_df = pd.concat([true_eval_df, downsampled_false_eval_df])\n","\n","X_eval_tensor = [torch.tensor(matrix, dtype=torch.float32) for matrix in eval_df['matrix']]\n","y_eval_tensor = torch.tensor(eval_df['label'].astype(int).values, dtype=torch.long)\n","\n","print(\"Resampled Evaluation Distribution of Labels (True for Alzheimer's, False for Other):\")\n","print(eval_df['label'].value_counts())\n","\n","# model hyperparams\n","input_size = X_train_tensor[0].shape[1]  # 21 dim\n","hidden_size = 32\n","num_layers = 2\n","\n","class AlzSeqLSTM(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers):\n","    super(AlzSeqLSTM, self).__init__()\n","    # LSTM layer\n","    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","    # Hidden layer\n","    self.hidden_layer = nn.Linear(hidden_size, hidden_size)  # A hidden layer before FC\n","    self.relu = nn.ReLU()  # Activation function for the hidden layer\n","    # Fully connected layer for classification\n","    self.fc = nn.Linear(hidden_size, 2)  # Output layer for binary classification\n","\n","  def forward(self, x):\n","    packed_input = rnn_utils.pack_sequence(x, enforce_sorted=False)\n","    packed_output, (h_n, c_n) = self.lstm(packed_input)\n","    lstm_out, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n","\n","    # hidden layer followed by ReLU\n","    hidden_out = self.hidden_layer(h_n[-1])\n","    hidden_out = self.relu(hidden_out)\n","\n","    # fully connected layer\n","    out = self.fc(hidden_out)\n","    return out\n","\n","\n","# init model\n","model = AlzSeqLSTM(input_size, hidden_size, num_layers)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n","\n","# train\n","num_epochs = 15\n","batch_size = 32\n","train_losses = []\n","eval_losses = []\n","train_accs = []\n","train_f1s = []\n","eval_accs = []\n","eval_f1s = []\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","  permutation = torch.randperm(len(X_train_tensor))\n","  epoch_loss = 0.0\n","  correct_pred = 0\n","  total_pred = 0\n","\n","  for i in range(0, len(X_train_tensor), batch_size):\n","    optimizer.zero_grad()\n","\n","    batch_indices = permutation[i:i + batch_size]\n","    batch_X = [X_train_tensor[idx] for idx in batch_indices]\n","    batch_y = y_train_tensor[batch_indices]\n","\n","    # feed forward\n","    outputs = model(batch_X)\n","    loss = criterion(outputs, batch_y)\n","    _, predicted = torch.max(outputs, 1)\n","    total_pred += batch_y.size(0)\n","    correct_pred += (predicted == batch_y).sum().item()\n","\n","\n","    # back prop\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  train_loss = epoch_loss / int(len(X_train_tensor)/batch_size)\n","  train_losses.append(train_loss)\n","  train_acc = correct_pred / total_pred\n","  train_accs.append(train_acc)\n","\n","  # validation loss\n","  model.eval()\n","  with torch.no_grad():\n","    eval_outputs = model(X_eval_tensor)\n","    # print(eval_outputs)\n","    # print(y_eval_tensor)\n","\n","    eval_loss = criterion(eval_outputs, y_eval_tensor).item()\n","    _, eval_predicted = torch.max(eval_outputs, 1)\n","    eval_acc = (eval_predicted == y_eval_tensor).sum().item() / len(y_eval_tensor)\n","    eval_losses.append(eval_loss)\n","    eval_accs.append(eval_acc)\n","\n","  print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}, Train Acc: {train_acc:.4f}, Eval Acc: {eval_acc:.4f}\")\n","\n","# train and valid losses\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(eval_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Train and Validation Loss')\n","plt.legend()\n","\n","# accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accs, label='Train Accuracy')\n","plt.plot(eval_accs, label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Train and Validation Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jp7gbs8MSbiJ","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":7,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n","\n","# evaluate\n","model.eval()\n","with torch.no_grad():\n","  outputs = model(X_eval_tensor)\n","  _, predicted = torch.max(outputs, 1)\n","  accuracy = (predicted == y_eval_tensor).sum().item() / len(y_eval_tensor)\n","  print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n","\n","  # confusion matrix\n","  cm = confusion_matrix(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy())\n","  print(f\"Confusion Matrix:\\n{cm}\")\n","\n","  # Precision, Recall, F1 Score\n","  precision = precision_score(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy(), pos_label=1)\n","  recall = recall_score(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy(), pos_label=1)\n","  f1 = f1_score(y_eval_tensor.cpu().numpy(), predicted.cpu().numpy(), pos_label=1)\n","\n","  print(f\"Precision: {precision:.4f}\")\n","  print(f\"Recall: {recall:.4f}\")\n","  print(f\"F1 Score: {f1:.4f}\")\n","\n","  # ROC and AUC calculation\n","  y_proba = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # probabilities for class 1\n","  fpr, tpr, _ = roc_curve(y_eval_tensor.cpu().numpy(), y_proba)\n","  roc_auc = auc(fpr, tpr)\n","\n","  # Plot ROC curve\n","  plt.figure()\n","  plt.rcParams.update({'font.size': 8})\n","\n","  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('Receiver Operating Characteristic (ROC)')\n","  plt.legend(loc=\"lower right\")\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asevpzIwStJ4","executionInfo":{"status":"aborted","timestamp":1730451786618,"user_tz":-660,"elapsed":7,"user":{"displayName":"Yufan Zhang","userId":"11266088522373789319"}}},"outputs":[],"source":["np.savez(file_path + 'results_pre_blood_best_roc.npz', fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1YGgd82TrrigEkbzHpHjFWoDniMwBmbqM","timestamp":1728637140347},{"file_id":"1rBjmAXcj0JQtkeQLL-Xv2nDILYDJzIEJ","timestamp":1726311548480},{"file_id":"1fQn_hfGqYrS4zxLWpGCH9QEiLirDyalm","timestamp":1726225595929},{"file_id":"1y9yvoUvOWhrbogJcY_59tyMpQiLTKbZM","timestamp":1724930392272},{"file_id":"1d1PYWzgqTst4wdd26X37AKQkOzLn-jVf","timestamp":1724894963036},{"file_id":"https://github.com/melbourne-cdth/comp90089_digital_phenotyping_tutorial/blob/main/digital_phenotyping_tutorial.ipynb","timestamp":1723694713941}],"collapsed_sections":["g6nsCG0nlEQi"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}